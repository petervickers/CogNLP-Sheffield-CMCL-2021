{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMCL ElasticNet",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex446SK-Y9Lk"
      },
      "source": [
        "!pip install wordfreq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzCjP5qcWYL3"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from collections.abc import Iterable\n",
        "from sklearn.svm import SVR\n",
        "from collections import Counter\n",
        "from wordfreq import zipf_frequency\n",
        "from string import punctuation\n",
        "\n",
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "nlp  = spacy.load('en_core_web_sm')  \n",
        "nlp.tokenizer = nlp.tokenizer.tokens_from_list\n",
        "\n",
        "pd.options.mode.chained_assignment = None "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqWly9_FWSGM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-GapWBiTfq_"
      },
      "source": [
        "# Define splits\n",
        "\n",
        "SPLIT_1 = (7839,10490,400,533)\n",
        "SPLIT_2 = (10490,13082,533,667)\n",
        "SPLIT_3 = (13082,15737,667,800)\n",
        "SPLIT_ALL = (15737,15737,800,800)\n",
        "\n",
        "USE_SPLIT=SPLIT_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RwyiwQuH1tt"
      },
      "source": [
        "# Helper Functions\n",
        "\n",
        "def pos_from_tokensized(sent):  \n",
        "  sent[-1] = sent[-1][:-1]    # Remove full-stop \n",
        "  sent = [word.replace(',', '') for word in sent]\n",
        "  doc = nlp(sent)\n",
        "  return([token.pos_ for token in doc])\n",
        "\n",
        "\n",
        "def add_seq_states(n_forward, n_backward, vars):\n",
        "  print_vars = []\n",
        "  for var in vars:\n",
        "    for forward_step in range(1, n_forward+1):\n",
        "      built_var = f'NEXT_{forward_step}_{var}'\n",
        "      print_vars.append(built_var)\n",
        "      data[built_var] = np.zeros_like(data[var])\n",
        "      for sample in range(len(data)):        \n",
        "        if sample+forward_step < len(data) and data['sentence_id'][sample] == data['sentence_id'][sample+forward_step]:\n",
        "          data[built_var][sample] = data[var][sample+forward_step]\n",
        "        else:\n",
        "          data[built_var][sample] = np.zeros_like(data[var][0]).tolist()\n",
        "\n",
        "    for backward_step in range(1, n_backward+1, 1):\n",
        "      built_var = f'PREV_{backward_step}_{var}'\n",
        "      print_vars.append(built_var)\n",
        "      data[built_var] = np.zeros_like(data[var])\n",
        "      for sample in range(len(data)):        \n",
        "        if sample-backward_step >= 0 and data['sentence_id'][sample] == data['sentence_id'][sample-backward_step]:\n",
        "          data[built_var][sample] = data[var][sample-backward_step]\n",
        "        else:\n",
        "          data[built_var][sample] = np.zeros_like(data[var][0]).tolist()\n",
        "\n",
        "  return(print_vars)\n",
        "\n",
        "def flatten(items):\n",
        "  for item in items:\n",
        "      if isinstance(item, list):\n",
        "          yield from flatten(item)\n",
        "      else:\n",
        "          yield item\n",
        "\n",
        "def plot_sent(sent, y, y_hat):\n",
        "  eg_results = {}\n",
        "  sent_len = len(sent)\n",
        "\n",
        "  for idx, target in enumerate(['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']):\n",
        "    eg_results[target] = y[:,idx]\n",
        "    eg_results['PRED_'+target] = y_hat[:,idx]\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  locs, labels = plt.xticks()  # Get the current locations and labels.\n",
        "  plt.xticks(list(range(sent_len)), sent, rotation=-90)  # Set text labels and properties.\n",
        "\n",
        "  plt.plot(list(range(sent_len)), eg_results['nFix'], color='darkblue', label = 'nFix')\n",
        "  plt.plot(list(range(sent_len)), eg_results['PRED_nFix'], '--', color='darkblue')\n",
        "\n",
        "  plt.plot(list(range(sent_len)), eg_results['FFD'], color='darkred', label = 'FFD')\n",
        "  plt.plot(list(range(sent_len)), eg_results['PRED_FFD'], '--', color='darkred')\n",
        "\n",
        "  plt.plot(list(range(sent_len)), eg_results['GPT'], color='darkorange', label = 'GPT')\n",
        "  plt.plot(list(range(sent_len)), eg_results['PRED_GPT'], '--', color='darkorange')\n",
        "\n",
        "  plt.plot(list(range(sent_len)), eg_results['TRT'], color='darkgreen', label = 'TRT')\n",
        "  plt.plot(list(range(sent_len)), eg_results['PRED_TRT'], '--', color='darkgreen')\n",
        "\n",
        "  plt.plot(list(range(sent_len)), eg_results['fixProp'], color='deeppink', label = 'fixProp')\n",
        "  plt.plot(list(range(sent_len)), eg_results['PRED_fixProp'], '--', color='deeppink')\n",
        "\n",
        "  #plt.yscale('symlog')\n",
        "\n",
        "  plt.legend(loc='upper right', ncol=len(eg_results))\n",
        "  plt.tight_layout()\n",
        "  plt.ylim((0,110))\n",
        "  plt.title('MAE '+str(float(\"{0:.5f}\".format(mean_absolute_error(y, y_hat)))))\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "def val_to_plot(val_df, preds):\n",
        "  sent_boundaries = []\n",
        "  sent_start = 0\n",
        "  y_val_sents = val_df['sentence_id'].tolist()\n",
        "  sent_id_tracker = y_val_sents[0]\n",
        "  for idx, sent_id in enumerate(y_val_sents):\n",
        "      if sent_id_tracker < sent_id or idx == len(y_val_sents) - 1:\n",
        "          if idx == len(data) - 1:\n",
        "                idx += 1\n",
        "          sent_boundaries.append((sent_start,idx))              \n",
        "          sent_start = deepcopy(idx)\n",
        "      sent_id_tracker = sent_id\n",
        "  \n",
        "  val_results = {}\n",
        "  val_store = {}\n",
        "\n",
        "  for idx, sent_span in enumerate(sent_boundaries):\n",
        "    \n",
        "    x = val_df.iloc[sent_span[0]:sent_span[1],:]['word'].tolist()\n",
        "    y = val_df.iloc[sent_span[0]:sent_span[1],:][['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']].values\n",
        "    y_hat = np.vstack([preds[target][sent_span[0]:sent_span[1]] for target in ['nFix',\t'FFD', 'GPT', 'TRT', 'fixProp']]).transpose()\n",
        "    assert y_hat.shape == y.shape\n",
        "    val_results[idx] = float(mean_absolute_error(y, y_hat))\n",
        "    val_store[idx] = [x, y, y_hat] \n",
        "\n",
        "  return(val_results, val_store)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbKFD5gLYRR9"
      },
      "source": [
        "!wget -O CMCL2021_train.zip  https://competitions.codalab.org/my/datasets/download/79675cf7-c344-478d-97f6-d4cfadb4a54a\n",
        "!unzip CMCL2021_train.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xswggp9GWeOC"
      },
      "source": [
        "#Load the data with some preprocessed additions\n",
        "\n",
        "train_data = pd.read_csv('training_data/training_data.csv')\n",
        "# Remove EOS as redundant by word_id\n",
        "train_data['word'] = train_data['word'].str.replace('<EOS>', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dANeJIc0b8vf"
      },
      "source": [
        "test_data = pd.read_csv('test_data/test_data.csv')\n",
        "test_data['word'] = test_data['word'].str.replace('<EOS>', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPWU5qtS8Rss"
      },
      "source": [
        "# Load NN Features\n",
        "embs = pd.read_pickle('Feature Enhancement/CMCL_XLNET_Fold_{str(FOLD)}_Embeddings.pkl')\n",
        "data['BERT_embed'] = [v[0].reshape(-1).tolist() for k, v in sorted(embs.items(), key=lambda item: item[0])]\n",
        "\n",
        "test_embs = pd.read_pickle('Feature Enhancement/CMCL_XLNET_Test_Embeddings.pkl')\n",
        "test_data['BERT_embed'] = [v[0].reshape(-1).tolist() for k, v in sorted(test_embs.items(), key=lambda item: item[0])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGf3MCUVyjHT"
      },
      "source": [
        "all_poses = []\n",
        "\n",
        "sent_start = 0\n",
        "word_id_tracker = 0\n",
        "\n",
        "for word_id_word in test_data[['word_id', 'word']].iterrows():\n",
        "  idx = word_id_word[0]\n",
        "  word_id, word = word_id_word[1].tolist()\n",
        "\n",
        "  if word_id_tracker > word_id:\n",
        "    sentence = test_data['word'].tolist()[sent_start:idx]\n",
        "    sent_poses = pos_from_tokensized(sentence)\n",
        "\n",
        "    assert len(sent_poses) == len(sentence)\n",
        "    all_poses.extend(sent_poses)\n",
        "    sent_start = deepcopy(idx)\n",
        "\n",
        "  word_id_tracker = word_id\n",
        "\n",
        "sentence = test_data['word'].tolist()[sent_start:]\n",
        "sent_poses = pos_from_tokensized(sentence)\n",
        "\n",
        "assert len(sent_poses) == len(sentence)\n",
        "all_poses.extend(sent_poses)\n",
        "\n",
        "test_data['pos'] = all_poses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9KAv7iwXo8b"
      },
      "source": [
        "# Make some variables categorical\n",
        "\n",
        "vars_to_cat = ['word', 'pos'] \n",
        "\n",
        "for var in vars_to_cat:\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  data[f'CAT_{var}'] = le.fit_transform(data[var])\n",
        "\n",
        "for var in vars_to_cat:\n",
        "  test_data[f'CAT_{var}'] = le.fit_transform(test_data[var])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB7g_5ndkExn"
      },
      "source": [
        "# Add EOS, Word Length, Word Frequency for Train\n",
        "\n",
        "is_EOS = []\n",
        "is_SOS = []\n",
        "\n",
        "for row in data.iterrows():\n",
        "  idx, row_data = row[0], row[1]\n",
        "\n",
        "  if idx != len(data)-1 and row_data['word_id'] < data.iloc[idx+1]['word_id']:\n",
        "    is_EOS.append(0)\n",
        "  else:\n",
        "    is_EOS.append(1)\n",
        "\n",
        "for row in data.iterrows():\n",
        "  idx, row_data = row[0], row[1]\n",
        "\n",
        "  if row_data['word_id'] == 0:\n",
        "    is_SOS.append(1)\n",
        "  else:\n",
        "    is_SOS.append(0)\n",
        "\n",
        "data['Is_EOS']=is_EOS\n",
        "data['Is_SOS']=is_SOS\n",
        "data['word_len'] = [len(word) for word in data['word']]\n",
        "data['zipf_frequency'] = [zipf_frequency(word.rstrip(punctuation), 'en', wordlist='large') for word in data['word']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-AhP-rGxjf3"
      },
      "source": [
        "# Add EOS, Word Length, Word Frequency for Test\n",
        "\n",
        "is_EOS = []\n",
        "is_SOS = []\n",
        "\n",
        "for row in test_data.iterrows():\n",
        "  idx, row_data = row[0], row[1]\n",
        "\n",
        "  if idx != len(test_data)-1 and row_data['word_id'] < test_data.iloc[idx+1]['word_id']:\n",
        "    is_EOS.append(0)\n",
        "  else:\n",
        "    is_EOS.append(1)\n",
        "\n",
        "for row in test_data.iterrows():\n",
        "  idx, row_data = row[0], row[1]\n",
        "\n",
        "  if row_data['word_id'] == 0:\n",
        "    is_SOS.append(1)\n",
        "  else:\n",
        "    is_SOS.append(0)\n",
        "\n",
        "test_data['Is_EOS']=is_EOS\n",
        "test_data['Is_SOS']=is_SOS\n",
        "test_data['word_len'] = [len(word) for word in test_data['word']]\n",
        "test_data['zipf_frequency'] = [zipf_frequency(word.rstrip(punctuation), 'en', wordlist='large') for word in test_data['word']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOlGuy_nxyeV"
      },
      "source": [
        "data[-10:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Td4hLH0J3ai"
      },
      "source": [
        "# Add features from previous/subsequent words\n",
        "\n",
        "print_vars = add_seq_states(n_forward=1, n_backward=1, vars=['BERT_embed', 'Is_EOS', 'Is_SOS', 'word_len', 'zipf_frequency', 'CAT_word'])\n",
        "print('Added:')\n",
        "print('\\'' + '\\', \\''.join(print_vars) + '\\'')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UySOPKFTvxQ5"
      },
      "source": [
        "data.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goaj_4O7_qX8"
      },
      "source": [
        "# Pick features to train with in 'feature_columns'\n",
        "\n",
        "feature_columns = ['word_len', 'zipf_frequency', 'CAT_pos', 'Is_EOS', 'Is_SOS', 'BERT_embed']\n",
        "\n",
        "\n",
        "X_train = np.vstack([list(flatten(x)) for x in data[feature_columns].values]).tolist()\n",
        "y_train = data[['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']]\n",
        "\n",
        "X_test = np.vstack([list(flatten(x)) for x in test_data[feature_columns].values]).tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68AsMTQO5duQ"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train_scale = scaler.transform(X_train)\n",
        "X_test_scale = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPCVYtE0_vML"
      },
      "source": [
        "# Run the task with the model specified\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "\n",
        "model = ElasticNetCV\n",
        "\n",
        "preds = {}\n",
        "total_mae = 0\n",
        "for target in ['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']:\n",
        "  print('fitting', target)\n",
        "  target_y_train = y_train[target]\n",
        "\n",
        "  reg = model(l1_ratio=[0.2,0.4,0.5,0.6,0.7,0.8], max_iter=20000).fit(X_train_scale, target_y_train)\n",
        "  \n",
        "  y_hat = reg.predict(X_test_scale)\n",
        "\n",
        "  y_hat[y_hat>100] = 100\n",
        "  y_hat[y_hat<0] = 0\n",
        "  preds[target] = y_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ9QPg6R0s2A"
      },
      "source": [
        "for target in ['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']: \n",
        "    test_data[target] = preds[target]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-WVcj0_zTcc"
      },
      "source": [
        "submit_test_df = test_data[['sentence_id', 'word_id', 'word', 'nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYURowM92NvO"
      },
      "source": [
        "submit_test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfJiuuqnm1vq"
      },
      "source": [
        "submit_test_df.to_csv('drive/Shareddrives/CMCL Shared Task - CogNLP@Sheffield/Predicitons/ElasticNetCV-alpha=0.6_word_len+zipf_freq+CAT_pos+Is_EOS+Is_SOS+XLNET_A.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl11GDe9zgRS"
      },
      "source": [
        "# Format validation results \n",
        "val_results, val_store = val_to_plot(submit_test_df, preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXmkdLA6pWfZ"
      },
      "source": [
        "for val_idx, score in Counter(val_results).most_common():\n",
        "    x, y, y_hat = val_store[val_idx]\n",
        "    plot_sent(x, y, y_hat)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkjfzTchpXII"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}