{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "CMCL XLNET",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1xj6OJgD6nh",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "# Edited from https://github.com/Kyubyong/nlp_made_easy/blob/master/Pos-tagging%20with%20Bert%20Fine-tuning.ipynb "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "ql8XfASKNgpd"
      },
      "source": [
        "import torch\n",
        "torch.manual_seed(0)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "import random\n",
        "random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eijXLlUkvMvJ",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJiZ5pkJvLpU",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "import os\n",
        "from string import punctuation\n",
        "from copy import deepcopy\n",
        "import pickle\n",
        "import os \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import FileLink\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils import data\n",
        "import torch.optim as optim\n",
        "from transformers import AutoTokenizer, AutoModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB-vAE9jvcOY",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "train_data = pd.read_csv('training_data/training_data.csv')\n",
        "# Remove EOS as redundant by word_id\n",
        "train_data['word'] = train_data['word'].str.replace('<EOS>', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcGtLIMz4WkF",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d35bZ5F_vlOh",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "X = []\n",
        "Y = []\n",
        "W = []\n",
        "I = []\n",
        "\n",
        "sent_start = 0\n",
        "word_id_tracker = 0\n",
        "for word_id_word in tqdm(train_data[['word_id', 'word']].iterrows(), \n",
        "                         total=len(train_data)):\n",
        "    idx = word_id_word[0]\n",
        "    word_id, word = word_id_word[1].tolist()\n",
        "    if word_id_tracker > word_id or idx == len(train_data) - 1:\n",
        "        if idx == len(train_data) - 1:\n",
        "              idx += 1\n",
        "        sentence = train_data['word'].tolist()[sent_start:idx]        \n",
        "        target = train_data[['nFix', 'FFD', 'GPT', 'TRT', 'fixProp']][sent_start:idx].values.tolist()\n",
        "        word_lengths = [len(word.rstrip(punctuation)) for word in sentence]\n",
        "        W.append(word_lengths)\n",
        "        X.append(sentence)\n",
        "        Y.append(target) \n",
        "        I.append((sent_start, idx))     \n",
        "      \n",
        "        sent_start = deepcopy(idx)\n",
        "    word_id_tracker = word_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X91RSBt3lpY2",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "#Train on val split 1,2,3 or all (for test predicitons)\n",
        "FOLD = 1 #1,2,3,'ALL'\n",
        "\n",
        "if FOLD == 1:\n",
        "  X_train, Y_train, W_train, I_train = X[:400]+X[533:], Y[:400]+Y[533:], W[:400]+W[533:], I[:400]+I[533:]\n",
        "  X_val, Y_val, W_val, I_val = X[400:533], Y[400:533], W[400:533], I[400:533]\n",
        "elif FOLD == 2:\n",
        "  X_train, Y_train, W_train, I_train = X[:533]+X[667:], Y[:533]+Y[667:], W[:533]+W[667:], I[:533]+I[667:]\n",
        "  X_val, Y_val, W_val, I_val = X[533:667], Y[533:667], W[533:667], I[533:667]\n",
        "elif FOLD == 3:\n",
        "  X_train, Y_train, W_train, I_train = X[:667], Y[:667], W[:667], I[:667]\n",
        "  X_val, Y_val, W_val, I_val = X[667:], Y[667:], W[667:], I[667:]\n",
        "elif FOLD == 'ALL':\n",
        "  X_train, Y_train, W_train, I_train = X, Y, W, I\n",
        "  X_val, Y_val, W_val, I_val = X[667:], Y[667:], W[667:], I[667:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ghv3gXKvzUz",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "class EyeTrackDataset(data.Dataset):\n",
        "    def __init__(self, sents_targets_lengths_idxes):\n",
        "        sents, target_vars, word_lengths, data_idxes  = [], [], [], []\n",
        "        for sent, target, lengths, idxes in sents_targets_lengths_idxes:\n",
        "            words = sent\n",
        "            sents.append(['[CLS]'] + words + ['[SEP]'])\n",
        "            target_vars.append([[0]*5] + target + [[0]*5])\n",
        "            word_lengths.append([0] + lengths + [0]) \n",
        "            data_idxes.append(idxes)\n",
        "        self.sents, self.target_vars, self.word_lengths, self.data_idxes = sents, target_vars, word_lengths, data_idxes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        words, targets, lengths, idxes = self.sents[idx], self.target_vars[idx], self.word_lengths[idx], self.data_idxes[idx]\n",
        "\n",
        "        w_lens, x, y = [], [], [] \n",
        "        # Indicate whether a sentence-piece token is the head or a word or not\n",
        "        is_heads = []\n",
        "\n",
        "        for w, t, l in zip(words, targets, lengths):\n",
        "            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n",
        "            tok_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "            # Mask non-head tokens, their targets, and their lengths\n",
        "            # Targets with value of 0 will not be used for backpropagation\n",
        "            is_head = [1] + [0]*(len(tokens) - 1) \n",
        "            t = [t] + [[0]*5] * (len(tokens) - 1)  \n",
        "            l = [l] * (len(tokens))  \n",
        "\n",
        "            x.extend(tok_ids)\n",
        "            is_heads.extend(is_head)\n",
        "            y.extend(t)\n",
        "            w_lens.extend(l)\n",
        "        assert len(w_lens)==len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n",
        "\n",
        "        seqlen = len(y)\n",
        "        words = \" \".join(words)\n",
        "\n",
        "        return words, x, is_heads, y, seqlen, w_lens, idxes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM4aGFRH0AeC",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "train_dataset = EyeTrackDataset(zip(X_train, Y_train, W_train, I_train))\n",
        "val_dataset   = EyeTrackDataset(zip(X_val, Y_val, W_val, I_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-D4gSPzxSrv",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "def pad(batch):\n",
        "    '''Pads to the longest sample'''\n",
        "    \n",
        "\n",
        "    f = lambda x: [sample[x] for sample in batch]\n",
        "    f_p0 = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch]\n",
        "    f_p1 = lambda x, seqlen: [sample[x] + [[0]*5] * (seqlen - len(sample[x])) for sample in batch]\n",
        "\n",
        "    seqlens = f(4)\n",
        "    maxlen = np.array(seqlens).max()\n",
        "\n",
        "    words = f(0)\n",
        "    x = f_p0(1, maxlen)\n",
        "    is_heads = f(2)\n",
        "    y = f_p1(3, maxlen)    \n",
        "    w_lens = f_p0(5, maxlen)\n",
        "    idxes = f(6)\n",
        "    \n",
        "    return words, torch.LongTensor(x), is_heads, torch.FloatTensor(y), seqlens, torch.LongTensor(w_lens), idxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZlnaqG63yid",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5sEOMRcxv_c",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained('xlnet-base-cased') \n",
        "        \n",
        "        self.regr1 = nn.Linear(769, 256)\n",
        "        self.regr2 = nn.Linear(256, 256)\n",
        "        self.regr3 = nn.Linear(256, 5)\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.return_attention = False\n",
        "\n",
        "    def freeze_bert(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "    def randomise_weights(self):\n",
        "        for module in self.bert.modules():\n",
        "            self.bert._init_weights(module)\n",
        "            \n",
        "    def forward(self, x, y, w_lens):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        l = w_lens.to(device)\n",
        "        \n",
        "        if self.training:\n",
        "            self.bert.train()\n",
        "            outputs = self.bert(x) #(batch_size, sequence_length, hidden_size)  \n",
        "            enc = outputs[0]    \n",
        "            hidden_states = outputs[0]             \n",
        "\n",
        "            lenc = torch.cat((enc, l.unsqueeze(-1), ), dim=2)\n",
        "            lenc = self.regr1(lenc)                        \n",
        "            lenc = self.regr2(lenc)\n",
        "            y_hat = self.regr3(lenc)\n",
        "        else:\n",
        "            self.bert.eval()\n",
        "            with torch.no_grad():                \n",
        "                outputs = self.bert(x) #(batch_size, sequence_length, hidden_size)  \n",
        "                enc = outputs[0]\n",
        "                hidden_states = outputs[0]\n",
        "\n",
        "                lenc = torch.cat((enc, l.unsqueeze(-1), ), dim=2)\n",
        "                lenc = self.regr1(lenc)                \n",
        "                lenc = self.regr2(lenc)\n",
        "                y_hat = self.regr3(lenc)                                    \n",
        "                \n",
        "        return y, y_hat, hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pWOOFkj_Ngpr"
      },
      "source": [
        "def get_hidden(model, iterator):\n",
        "    word_hidden_states = {}\n",
        "    model.eval()    \n",
        "   \n",
        "    batch_offset = 0\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            words, x, is_heads, y, seqlens, w_lens, idxes = batch\n",
        "            y_hat, lenc, hidden_states = model(x, y, w_lens)  # y_hat: (N, T)\n",
        "\n",
        "            for sample_idx, (sample_heads, idxes, words) in enumerate(zip(is_heads, idxes, words)):     \n",
        "                \n",
        "                # pick attentions from word heads \n",
        "                take_idxes_raw = list(range(len(sample_heads)))                \n",
        "                take_idxes = [take_idx for idx, take_idx in enumerate(take_idxes_raw) if sample_heads[idx]==1][1:-1]               \n",
        "\n",
        "                per_word_hidden_state = hidden_states[sample_idx, take_idxes, :].cpu().numpy()\n",
        "                per_word_hidden_state_split = np.split(per_word_hidden_state, per_word_hidden_state.shape[0], axis=0)\n",
        "\n",
        "                assert len(per_word_hidden_state_split) == idxes[1]-idxes[0]\n",
        "\n",
        "                for idx, word_hidn in zip(range(idxes[0], idxes[1]), per_word_hidden_state_split):\n",
        "                    word_hidden_states[idx] = word_hidn\n",
        "\n",
        "    return(word_hidden_states)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzwMVhzA0xGu",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "MAX_SEQ_LEN = 100\n",
        "DEV_BATCH_SIZE = 32\n",
        "\n",
        "def eval(model, iterator):\n",
        "    model.eval()\n",
        "    \n",
        "    batch_offset = 0\n",
        "    Words, Is_heads, Y, Y_hat, Y_hat_raw = [], [], np.empty((0, 5)), np.empty((0, 5)), np.empty((0, MAX_SEQ_LEN, 5))\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            words, x, is_heads, y, seqlens, w_lens, idxes = batch\n",
        "\n",
        "            _, y_hat, _ = model(x, y, w_lens)  # y_hat: (N, T)            \n",
        "            \n",
        "            Y_hat_raw = np.vstack((Y_hat_raw, np.zeros((y_hat.shape[0], MAX_SEQ_LEN, 5))))\n",
        "\n",
        "            for head_idx, head_list in enumerate(is_heads):\n",
        "              place_idx = 0              \n",
        "              for take_idx, word_ind in enumerate(head_list):\n",
        "\n",
        "                # Ignore CLS and SEP predictions.\n",
        "                if take_idx in (0, len(head_list)-1):\n",
        "                  pass\n",
        "                elif word_ind == 1:\n",
        "                  Y_hat_raw[head_idx+batch_offset,place_idx,:] = y_hat.cpu().numpy()[head_idx,take_idx,:]  \n",
        "                  place_idx += 1     \n",
        "                # Ignore 0ed token indexes           \n",
        "                elif word_ind == 0:\n",
        "                  pass\n",
        "\n",
        "            batch_offset += y_hat.shape[0]\n",
        "\n",
        "            y_useful = y[y.nonzero(as_tuple=True)]\n",
        "            y_hat_useful = y_hat.squeeze()[y.nonzero(as_tuple=True)]\n",
        "            \n",
        "            Words.extend(words)\n",
        "            Is_heads.extend(is_heads)            \n",
        "            Y = np.vstack((Y, y_useful.numpy().reshape(-1,5)))\n",
        "            Y_hat = np.vstack((Y_hat, y_hat_useful.squeeze().cpu().numpy().reshape(-1,5)))\n",
        "          \n",
        "        MAE_Overall = mean_absolute_error(Y, Y_hat)         \n",
        "        MAE_nFix = mean_absolute_error(Y[:,0], Y_hat[:,0])  \n",
        "        MAE_FFD = mean_absolute_error(Y[:,1], Y_hat[:,1])  \n",
        "        MAE_GPT = mean_absolute_error(Y[:,2], Y_hat[:,2])  \n",
        "        MAE_TRT = mean_absolute_error(Y[:,3], Y_hat[:,3])  \n",
        "        MAE_fixProp = mean_absolute_error(Y[:,4], Y_hat[:,4])\n",
        "        MAEs = {'Overall': MAE_Overall, 'nFix': MAE_nFix, 'FFD': MAE_FFD,\t\n",
        "                'GPT': MAE_GPT, 'TRT': MAE_TRT, 'fixProp': MAE_fixProp}\n",
        "\n",
        "    return(Y, Y_hat_raw, MAEs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJZqXmYO0BSi",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "def train(model, iterator, test_iter, optimizer, criterion, epochs):\n",
        "    losses = []\n",
        "    steps = 0\n",
        "    for j in range(epochs):  \n",
        "        model.train()     \n",
        "\n",
        "        for i, batch in enumerate(iterator):            \n",
        "            steps += 1\n",
        "            words, x, is_heads, y, seqlens, w_lens, idxes = batch\n",
        "            optimizer.zero_grad()            \n",
        "           \n",
        "            y, y_hat, _ = model(x, y, w_lens) \n",
        "            y_hat = y_hat.squeeze() \n",
        "\n",
        "            # Don't use 0ed targets in calculating loss\n",
        "            loss = criterion(y_hat[y.nonzero(as_tuple=True)], y[y.nonzero(as_tuple=True)])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        \n",
        "        _, _, MAEs = eval(model, test_iter)\n",
        "        losses.append(MAEs[\"Overall\"])\n",
        "        print(f'epoch {j}, train loss: {float(\"{0:.5f}\".format(loss))}, Avg MAE: {float(\"{0:.5f}\".format(MAEs[\"Overall\"]))}, Moving Avg {float(\"{0:.5f}\".format(np.mean(losses[-10:])))}')\n",
        "        print(f'         nFix: {float(\"{0:.3f}\".format(MAEs[\"nFix\"]))} FFD: {float(\"{0:.3f}\".format(MAEs[\"FFD\"]))} GPT: {float(\"{0:.3f}\".format(MAEs[\"GPT\"]))} TRT:{float(\"{0:.3f}\".format(MAEs[\"TRT\"]))} fixProp:{float(\"{0:.3f}\".format(MAEs[\"fixProp\"]))}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NGti1NbiNgps"
      },
      "source": [
        "model = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ff9QFYPINgps"
      },
      "source": [
        "#model.randomise_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zezC9iZE0JFM",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "model.to(device)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSCFEBOa0NTo",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "train_iter = data.DataLoader(dataset=train_dataset,\n",
        "                             batch_size=32,\n",
        "                             shuffle=True,\n",
        "                             num_workers=1,\n",
        "                             collate_fn=pad)\n",
        "val_iter = data.DataLoader(dataset=val_dataset,\n",
        "                             batch_size=DEV_BATCH_SIZE,\n",
        "                             shuffle=False,\n",
        "                             num_workers=1,\n",
        "                             collate_fn=pad)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aQAcZXZsxM5",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "N_EPOCHS = 10\n",
        "optimizer = optim.AdamW(model.parameters(), lr = 0.00001)\n",
        "criterion = nn.SmoothL1Loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJfaGgQY0Usz",
        "trusted": true,
        "scrolled": false
      },
      "source": [
        "train(model, train_iter, val_iter, optimizer, criterion, N_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Dxd469itLV",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "hidden_states_train = get_hidden(model, train_iter)\n",
        "hidden_states_val = get_hidden(model, val_iter)\n",
        "\n",
        "hidden_states_all = {**hidden_states_train, **hidden_states_val}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "Cp79VMHrNgpv"
      },
      "source": [
        "output = open(f'CMCL_XLNET_Fold_{str(FOLD)}_Embeddings.pkl', 'wb')\n",
        "pickle.dump(hidden_states_val, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NfYLTu_GNgpv"
      },
      "source": [
        "torch.save(model.state_dict(), f'XLNET_Fold_{str(FOLD)}.mdl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gQr65FSGNgpw"
      },
      "source": [
        "test_data = pd.read_csv('test_data/test_data.csv')\n",
        "test_data['word'] = test_data['word'].str.replace('<EOS>', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "e4SVS1zZNgpx"
      },
      "source": [
        "X_test = []\n",
        "Y_test = []\n",
        "W_test = []\n",
        "I_test = []\n",
        "\n",
        "\n",
        "sent_start = 0\n",
        "word_id_tracker = 0\n",
        "for word_id_word in tqdm(test_data[['word_id', 'word']].iterrows(), \n",
        "                         total=len(test_data)):\n",
        "    idx = word_id_word[0]\n",
        "    word_id, word = word_id_word[1].tolist()\n",
        "    if word_id_tracker > word_id or idx == len(test_data) - 1:\n",
        "        if idx == len(test_data) - 1:\n",
        "              idx += 1\n",
        "        sentence = test_data['word'].tolist()[sent_start:idx]        \n",
        "        word_lengths = [len(word.rstrip(punctuation)) for word in sentence]\n",
        "        W_test.append(word_lengths)\n",
        "        Y_test.append([[1]*5 for word in sentence])\n",
        "        X_test.append(sentence)\n",
        "        I_test.append((sent_start, idx))     \n",
        "      \n",
        "        sent_start = deepcopy(idx)\n",
        "    word_id_tracker = word_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WXD9ZiTlNgpy"
      },
      "source": [
        "test_dataset = EyeTrackDataset(zip(X_test, Y_test, W_test, I_test))\n",
        "\n",
        "test_iter = data.DataLoader(dataset=test_dataset,\n",
        "                             batch_size=DEV_BATCH_SIZE,\n",
        "                             shuffle=False,\n",
        "                             num_workers=1,\n",
        "                             collate_fn=pad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "54-CmD34Ngpy"
      },
      "source": [
        "hidden_states_test = get_hidden(model, test_iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dKEYp8sZNgpy"
      },
      "source": [
        "output = open('CMCL_XLNET_Test_Embeddings.pkl', 'wb')\n",
        "pickle.dump(hidden_states_test, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZo29Ov6s8KX",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "y, Y_hat_raw, _ = eval(model, val_iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tV1Ypt8s-zf",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "def plot_sent(sent, y, y_hat):\n",
        "  eg_results = {}\n",
        "  sent_len = len(sent)\n",
        "\n",
        "  for idx, target in enumerate(['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']):\n",
        "    eg_results[target] = y[:,idx]\n",
        "    eg_results['PRED_'+target] = y_hat[:,idx]\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  locs, labels = plt.xticks()  # Get the current locations and labels.\n",
        "  plt.xticks(list(range(sent_len)), sent, rotation=-90)  # Set text labels and properties.\n",
        "\n",
        "  plt.plot(list(range(sent_len)), eg_results['nFix'], color='darkblue', label = 'nFix')\n",
        "  plt.plot(list(range(sent_len)), eg_results['PRED_nFix'], '--', color='darkblue')\n",
        "\n",
        "  plt.plot(list(range(sent_len)), eg_results['FFD'], color='darkred', label = 'FFD')\n",
        "  plt.plot(list(range(sent_len)), eg_results['PRED_FFD'], '--', color='darkred')\n",
        "\n",
        "  plt.plot(list(range(sent_len)), eg_results['GPT'], color='darkorange', label = 'GPT')\n",
        "  plt.plot(list(range(sent_len)), eg_results['PRED_GPT'], '--', color='darkorange')\n",
        "\n",
        "  plt.plot(list(range(sent_len)), eg_results['TRT'], color='darkgreen', label = 'TRT')\n",
        "  plt.plot(list(range(sent_len)), eg_results['PRED_TRT'], '--', color='darkgreen')\n",
        "\n",
        "  plt.plot(list(range(sent_len)), eg_results['fixProp'], color='deeppink', label = 'fixProp')\n",
        "  plt.plot(list(range(sent_len)), eg_results['PRED_fixProp'], '--', color='deeppink')\n",
        "\n",
        "  #plt.yscale('symlog')\n",
        "\n",
        "  plt.legend(loc='upper right', ncol=len(eg_results))\n",
        "  plt.tight_layout()\n",
        "  plt.ylim((0,110))\n",
        "  plt.title('MAE '+str(float(\"{0:.5f}\".format(mean_absolute_error(y, y_hat)))))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udEbUae4s-9S",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "dev_results = {}\n",
        "dev_store = {}\n",
        "\n",
        "for idx, (x, y, y_hat) in enumerate(list(zip(X_val, Y_val, Y_hat_raw))): \n",
        "  dev_results[idx] = mean_absolute_error(np.array(y), y_hat[:np.array(y).shape[0],:])\n",
        "  dev_store[idx] = [x, np.array(y), y_hat[:np.array(y).shape[0],:]]    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pICM4e1Ns_RQ",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "# Worst to Best\n",
        "for val_idx, score in Counter(dev_results).most_common():\n",
        "    x, y, y_hat = dev_store[val_idx]\n",
        "    plot_sent(x, y, y_hat)\n",
        "    print(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pa7iGL-ZNgp0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}