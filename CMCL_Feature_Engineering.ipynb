{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMCL Feature Engineering",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex446SK-Y9Lk"
      },
      "source": [
        "!pip install wordfreq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzCjP5qcWYL3"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from collections.abc import Iterable\n",
        "from sklearn.svm import SVR\n",
        "from collections import Counter\n",
        "from wordfreq import word_frequency, zipf_frequency\n",
        "from string import punctuation\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "\n",
        "pd.options.mode.chained_assignment = None "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-GapWBiTfq_"
      },
      "source": [
        "# Define splits\n",
        "\n",
        "SPLIT_1 = (7839,10490,400,533)\n",
        "SPLIT_2 = (10490,13082,533,667)\n",
        "SPLIT_3 = (13082,15737,667,800)\n",
        "SPLIT_ALL = (15737,15737,800,800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPDewJ_AVG-5"
      },
      "source": [
        "USE_SPLIT = SPLIT_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RwyiwQuH1tt"
      },
      "source": [
        "# Helper Functions\n",
        "\n",
        "\n",
        "def add_seq_states(n_forward, n_backward, vars):\n",
        "  print_vars = []\n",
        "  for var in tqdm(vars):\n",
        "    for forward_step in range(1, n_forward+1):\n",
        "      built_var = f'NEXT_{forward_step}_{var}'\n",
        "      print_vars.append(built_var)\n",
        "      data[built_var] = np.zeros_like(data[var])\n",
        "      for sample in range(len(data)):        \n",
        "        if sample+forward_step < len(data) and data['sentence_id'][sample] == data['sentence_id'][sample+forward_step]:\n",
        "          data[built_var][sample] = data[var][sample+forward_step]\n",
        "        else:\n",
        "          data[built_var][sample] = np.zeros_like(data[var][0]).tolist()\n",
        "\n",
        "    for backward_step in range(1, n_backward+1, 1):\n",
        "      built_var = f'PREV_{backward_step}_{var}'\n",
        "      print_vars.append(built_var)\n",
        "      data[built_var] = np.zeros_like(data[var])\n",
        "      for sample in range(len(data)):        \n",
        "        if sample-backward_step >= 0 and data['sentence_id'][sample] == data['sentence_id'][sample-backward_step]:\n",
        "          data[built_var][sample] = data[var][sample-backward_step]\n",
        "        else:\n",
        "          data[built_var][sample] = np.zeros_like(data[var][0]).tolist()\n",
        "\n",
        "  return(print_vars)\n",
        "\n",
        "def add_seq_states_test(n_forward, n_backward, vars):\n",
        "  print_vars = []\n",
        "  for var in tqdm(vars):\n",
        "    for forward_step in range(1, n_forward+1):\n",
        "      built_var = f'NEXT_{forward_step}_{var}'\n",
        "      print_vars.append(built_var)\n",
        "      test_data[built_var] = np.zeros_like(test_data[var])\n",
        "      for sample in range(len(test_data)):        \n",
        "        if sample+forward_step < len(test_data) and test_data['sentence_id'][sample] == test_data['sentence_id'][sample+forward_step]:\n",
        "          test_data[built_var][sample] = test_data[var][sample+forward_step]\n",
        "        else:\n",
        "          test_data[built_var][sample] = np.zeros_like(test_data[var][0]).tolist()\n",
        "\n",
        "    for backward_step in range(1, n_backward+1, 1):\n",
        "      built_var = f'PREV_{backward_step}_{var}'\n",
        "      print_vars.append(built_var)\n",
        "      test_data[built_var] = np.zeros_like(test_data[var])\n",
        "      for sample in range(len(test_data)):        \n",
        "        if sample-backward_step >= 0 and test_data['sentence_id'][sample] == test_data['sentence_id'][sample-backward_step]:\n",
        "          test_data[built_var][sample] = test_data[var][sample-backward_step]\n",
        "        else:\n",
        "          test_data[built_var][sample] = np.zeros_like(test_data[var][0]).tolist()\n",
        "\n",
        "  return(print_vars)\n",
        "\n",
        "\n",
        "def flatten(items):\n",
        "  for item in items:\n",
        "      if isinstance(item, list):\n",
        "          yield from flatten(item)\n",
        "      else:\n",
        "          yield item\n",
        "\n",
        "def plot_sent(sent, y, y_hat):\n",
        "  eg_results = {}\n",
        "  sent_len = len(sent)\n",
        "\n",
        "  for idx, target in enumerate(['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']):\n",
        "    eg_results[target] = y[:,idx]\n",
        "    eg_results['PRED_'+target] = y_hat[:,idx]\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  locs, labels = plt.xticks()  # Get the current locations and labels.\n",
        "  plt.xticks(list(range(sent_len)), sent, rotation=-90)  # Set text labels and properties.\n",
        "\n",
        "  plt.plot(list(range(sent_len)), eg_results['nFix'], color='darkblue', label = 'nFix')\n",
        "  plt.plot(list(range(sent_len)), eg_results['PRED_nFix'], '--', color='darkblue')\n",
        "\n",
        "  plt.plot(list(range(sent_len)), eg_results['FFD'], color='darkred', label = 'FFD')\n",
        "  plt.plot(list(range(sent_len)), eg_results['PRED_FFD'], '--', color='darkred')\n",
        "\n",
        "  plt.plot(list(range(sent_len)), eg_results['GPT'], color='darkorange', label = 'GPT')\n",
        "  plt.plot(list(range(sent_len)), eg_results['PRED_GPT'], '--', color='darkorange')\n",
        "\n",
        "  plt.plot(list(range(sent_len)), eg_results['TRT'], color='darkgreen', label = 'TRT')\n",
        "  plt.plot(list(range(sent_len)), eg_results['PRED_TRT'], '--', color='darkgreen')\n",
        "\n",
        "  plt.plot(list(range(sent_len)), eg_results['fixProp'], color='deeppink', label = 'fixProp')\n",
        "  plt.plot(list(range(sent_len)), eg_results['PRED_fixProp'], '--', color='deeppink')\n",
        "\n",
        "  #plt.yscale('symlog')\n",
        "\n",
        "  plt.legend(loc='upper right', ncol=len(eg_results))\n",
        "  plt.tight_layout()\n",
        "  plt.ylim((0,110))\n",
        "  plt.title('MAE '+str(float(\"{0:.5f}\".format(mean_absolute_error(y, y_hat)))))\n",
        "  plt.show()\n",
        "\n",
        "def val_to_plot(val_df, preds):\n",
        "  sent_boundaries = []\n",
        "  sent_start = 0\n",
        "  y_val_sents = val_df['sentence_id'].tolist()\n",
        "  sent_id_tracker = y_val_sents[0]\n",
        "  for idx, sent_id in enumerate(y_val_sents):\n",
        "      if sent_id_tracker < sent_id or idx == len(y_val_sents) - 1:\n",
        "          if idx == len(data) - 1:\n",
        "                idx += 1\n",
        "          sent_boundaries.append((sent_start,idx))              \n",
        "          sent_start = deepcopy(idx)\n",
        "      sent_id_tracker = sent_id\n",
        "  \n",
        "  val_results = {}\n",
        "  val_store = {}\n",
        "\n",
        "  for idx, sent_span in enumerate(sent_boundaries):\n",
        "    \n",
        "    x = val_df.iloc[sent_span[0]:sent_span[1],:]['word'].tolist()\n",
        "    y = val_df.iloc[sent_span[0]:sent_span[1],:][['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']].values\n",
        "    y_hat = np.vstack([preds[target][sent_span[0]:sent_span[1]] for target in ['nFix',\t'FFD', 'GPT', 'TRT', 'fixProp']]).transpose()\n",
        "    assert y_hat.shape == y.shape\n",
        "    val_results[idx] = float(mean_absolute_error(y, y_hat))\n",
        "    val_store[idx] = [x, y, y_hat] \n",
        "\n",
        "  return(val_results, val_store)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFY8DwogswVN"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C4-DPAUtH3H"
      },
      "source": [
        "shared_dir = 'drive/Shareddrives/CMCL Shared Task - CogNLP@Sheffield/'\n",
        "local_dir = shared_dir+'Final models/local/'\n",
        "features_dir = shared_dir+'Feature Enhancement/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xswggp9GWeOC"
      },
      "source": [
        "#Load the data with some preprocessed additions\n",
        "\n",
        "data = pd.read_pickle(shared_dir+'Data/CMCL_Proc.pkl')\n",
        "#data = data.drop('BERT_embed', axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Myme8evXr7nQ"
      },
      "source": [
        "test_data = pd.read_csv(local_dir+'test_data_with_pos.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ-H4whQr7nR"
      },
      "source": [
        "test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OomxTlJPXVp-"
      },
      "source": [
        "## Example adding extra features from drive\n",
        "\n",
        "Conc_M = pd.read_excel(features_dir+'Conc.M_draft.xlsx').fillna(3)\n",
        "Conc_SD = pd.read_excel(features_dir+'Conc.SD_draft.xlsx')\n",
        "Conc_SD['Conc.SD'] = Conc_SD['Conc.SD'].fillna(Conc_SD['Conc.SD'].mean())\n",
        "Percent_Known = pd.read_excel(features_dir+'Percent_known.xlsx').fillna(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyNY4xKWr7nS"
      },
      "source": [
        "Conc_M_test = pd.read_excel(features_dir+'Conc.M_test_data.xlsx').fillna(3)\n",
        "Conc_SD_test = pd.read_excel(features_dir+'Conc.SD_test_data.xlsx')\n",
        "Conc_SD_test['Conc.SD'] = Conc_SD_test['Conc.SD'].fillna(Conc_SD_test['Conc.SD'].mean())\n",
        "Percent_Known_test = pd.read_excel(features_dir+'Percent_known_test_data.xlsx').fillna(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOUPu75Pr7nS"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1md9zQkqTOD"
      },
      "source": [
        "data['Conc_M'] = Conc_M['Conc.M']\n",
        "data['Conc_SD'] = Conc_SD['Conc.SD']\n",
        "data['Percent_Known'] = Percent_Known['Percent_known']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CISGVzfr7nT"
      },
      "source": [
        "test_data['Conc_M'] = Conc_M_test['Conc.M']\n",
        "test_data['Conc_SD'] = Conc_SD_test['Conc.SD']\n",
        "test_data['Percent_Known'] = Percent_Known_test['Percent_known']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S1SZXUjKlWx"
      },
      "source": [
        "MWE_Exists = pd.read_excel(local_dir+'mwe_features_with_comp_weights_embeddings_coarse_cats.xlsx')\n",
        "MWE_Exists_test = pd.read_excel(local_dir+'test_mwe_features_with_comp_weights_embeddings_coarse_cats.xlsx')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTcyE2j_KsM5"
      },
      "source": [
        "data['is_mwe'] = MWE_Exists['is_mwe']\n",
        "data['mwe_cat'] = MWE_Exists['mwe_cat']\n",
        "data['comp_score'] = MWE_Exists['comp']\n",
        "data['comp_weights'] = MWE_Exists['weights']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTPqwSHsr7nU"
      },
      "source": [
        "test_data['is_mwe'] = MWE_Exists_test['is_mwe']\n",
        "test_data['mwe_cat'] = MWE_Exists_test['mwe_cat']\n",
        "test_data['comp_score'] = MWE_Exists_test['comp']\n",
        "test_data['comp_weights'] = MWE_Exists_test['weights']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz-I7F5vw2zJ"
      },
      "source": [
        "embed_dict = {} \n",
        "for idx, row in MWE_Exists.iterrows():\n",
        "  embed_string = row['w2v_embedding']\n",
        "  embed = [float(val) for val in embed_string.strip('[]\\n').split()]\n",
        "  embed_dict[idx] = embed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDh5dA1ur7nU"
      },
      "source": [
        "test_embed_dict = {} \n",
        "for idx, row in MWE_Exists_test.iterrows():\n",
        "  embed_string = row['w2v_embedding']\n",
        "  embed = [float(val) for val in embed_string.strip('[]\\n').split()]\n",
        "  test_embed_dict[idx] = embed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK-DlHmfxiQ3"
      },
      "source": [
        "data['w2v_embedding'] = data.index.map(embed_dict)\n",
        "test_data['w2v_embedding'] = test_data.index.map(test_embed_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHSiZqH3r7nU"
      },
      "source": [
        "test_data['word'] = test_data['word'].str.replace('<EOS>', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iuk8hZbXr7nV"
      },
      "source": [
        "data.append(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9KAv7iwXo8b"
      },
      "source": [
        "# Make some variables categorical\n",
        "\n",
        "vars_to_cat = ['pos', 'word'] \n",
        "\n",
        "for var in vars_to_cat:\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  le.fit(data.append(test_data)[var])\n",
        "  data[f'CAT_{var}'] = le.transform(data[var])\n",
        "  test_data[f'CAT_{var}'] = le.transform(test_data[var])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5Zduq5OxrSh"
      },
      "source": [
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB7g_5ndkExn"
      },
      "source": [
        "# Add EOS, Word Length, Word Frequency\n",
        "\n",
        "is_EOS = []\n",
        "is_SOS = []\n",
        "\n",
        "for row in data.iterrows():\n",
        "  idx, row_data = row[0], row[1]\n",
        "\n",
        "  if idx != len(data)-1 and row_data['word_id'] < data.iloc[idx+1]['word_id']:\n",
        "    is_EOS.append(0)\n",
        "  else:\n",
        "    is_EOS.append(1)\n",
        "data['Is_EOS']=is_EOS\n",
        "\n",
        "for row in data.iterrows():\n",
        "  idx, row_data = row[0], row[1]\n",
        "\n",
        "  if row_data['word_id'] == 0:\n",
        "    is_SOS.append(1)\n",
        "  else:\n",
        "    is_SOS.append(0)\n",
        "\n",
        "data['Is_SOS']=is_SOS\n",
        "\n",
        "data['word_len'] = [len(word) for word in data['word']]\n",
        "\n",
        "data['zipf_frequency'] = [zipf_frequency(word.rstrip(punctuation), 'en', wordlist='large') for word in data['word']]\n",
        "data['word_frequency'] =  [word_frequency(word.rstrip(punctuation), 'en') for word in data['word']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2MP9HFGr7nW"
      },
      "source": [
        "# Add EOS, Word Length, Word Frequency\n",
        "\n",
        "is_EOS = []\n",
        "is_SOS = []\n",
        "\n",
        "for row in test_data.iterrows():\n",
        "  idx, row_data = row[0], row[1]\n",
        "\n",
        "  if idx != len(test_data)-1 and row_data['word_id'] < test_data.iloc[idx+1]['word_id']:\n",
        "    is_EOS.append(0)\n",
        "  else:\n",
        "    is_EOS.append(1)\n",
        "test_data['Is_EOS']=is_EOS\n",
        "\n",
        "for row in test_data.iterrows():\n",
        "  idx, row_data = row[0], row[1]\n",
        "\n",
        "  if row_data['word_id'] == 0:\n",
        "    is_SOS.append(1)\n",
        "  else:\n",
        "    is_SOS.append(0)\n",
        "\n",
        "test_data['Is_SOS']=is_SOS\n",
        "\n",
        "test_data['word_len'] = [len(word) for word in test_data['word']]\n",
        "\n",
        "test_data['zipf_frequency'] = [zipf_frequency(word.rstrip(punctuation), 'en', wordlist='large') for word in test_data['word']]\n",
        "test_data['word_frequency'] =  [word_frequency(word.rstrip(punctuation), 'en') for word in test_data['word']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xwOGztwr7nW"
      },
      "source": [
        "test_data[56:80]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyzqQZT7QFmA"
      },
      "source": [
        "saccade_len_dict = {1 : 0, 2: 0, 3: 0, 4: 1, 5: 1, 6:1, 7:1, 8:2, 9:2, 10:2}\n",
        "for i in range(11,27): \n",
        "  saccade_len_dict[i] = 3\n",
        "\n",
        "saccade_len_dict_binary = {i : (1 if i > 3 else 0) for i in range(1,data['word_len'].max()+1)}\n",
        "data['saccade_cat_binary'] = data['word_len'].map(saccade_len_dict_binary)\n",
        "data['saccade_cat'] = data['word_len'].map(saccade_len_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuPjcex5r7nW"
      },
      "source": [
        "test_data['saccade_cat_binary'] = test_data['word_len'].map(saccade_len_dict_binary)\n",
        "test_data['saccade_cat'] = test_data['word_len'].map(saccade_len_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JzpXG3A2uN4"
      },
      "source": [
        "typical_words = re.compile(r'[\"\\'\\(\\)\\[\\]]*[A-Za-z]*[\\'-.]?[A-Za-z]*[\\.\\-\\'\\\"?!\\(\\)\\[\\]:;,]*$|&|â€“|[0-9]+\\-year-old|[A-Za-z]*-in-law[\\.\\-\\'\\\"?!\\(\\)\\[\\]:;,%]*')\n",
        "typical_nums = re.compile(r'[\\.\\-\\'\\\"?!\\(\\)\\[\\]:;,]*[0-9]+[\\.\\-\\'\\\"?!\\(\\)\\[\\]:;,%]*[0-9]*(th|st|rd|nd)?[\\'s]?[\\.\\-\\'\\\"?!\\(\\)\\[\\]:;,%]*')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIhvPzJZ2pKY"
      },
      "source": [
        "strange_dict = {} \n",
        "for i, row in data.iterrows():\n",
        "  chars_match = re.match(typical_words, row['word'].strip())\n",
        "  nums_match = re.match(typical_nums, row['word'].strip())\n",
        "  if chars_match is not None: \n",
        "    if chars_match.group() == row['word']:\n",
        "      strange_dict[i] = 0 \n",
        "    else:\n",
        "      strange_dict[i] = 1\n",
        "  elif nums_match is not None: \n",
        "    if nums_match.group() == row['word']:\n",
        "      strange_dict[i] = 0 \n",
        "    else:\n",
        "      strange_dict[i] = 1\n",
        "  else:\n",
        "    strange_dict[i] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFg8RS6Z5aJq"
      },
      "source": [
        "data['is_strange'] = data.index.map(strange_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX2O659dr7nX"
      },
      "source": [
        "strange_dict = {} \n",
        "for i, row in test_data.iterrows():\n",
        "  chars_match = re.match(typical_words, row['word'].strip())\n",
        "  nums_match = re.match(typical_nums, row['word'].strip())\n",
        "  if chars_match is not None: \n",
        "    if chars_match.group() == row['word']:\n",
        "      strange_dict[i] = 0 \n",
        "    else:\n",
        "      strange_dict[i] = 1\n",
        "  elif nums_match is not None: \n",
        "    if nums_match.group() == row['word']:\n",
        "      strange_dict[i] = 0 \n",
        "    else:\n",
        "      strange_dict[i] = 1\n",
        "  else:\n",
        "    strange_dict[i] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pISwTEfRr7nX"
      },
      "source": [
        "test_data['is_strange'] = test_data.index.map(strange_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sA30Xfl6XmZ"
      },
      "source": [
        "for i, row in test_data.iterrows():\n",
        "  if np.isnan(row['is_strange']):\n",
        "    print(row['word'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FvSCkLSr7nX"
      },
      "source": [
        "geco_FFD = pd.read_csv(local_dir+'zuco_featureset-geco_FFD_word_level.csv').drop(['sentence_id','word_id', 'word'], axis=1)\n",
        "geco_TRT = pd.read_csv(local_dir+'zuco_featureset-geco_TRT_word_level.csv').drop(['sentence_id','word_id', 'word'], axis=1)\n",
        "geco_GPT = pd.read_csv(local_dir+'zuco_featureset-geco_GPT_word_level.csv').drop(['sentence_id','word_id', 'word'], axis=1)\n",
        "geco_nFix = pd.read_csv(local_dir+'zuco_featureset-geco_nFix_word_level.csv').drop(['sentence_id','word_id', 'word'], axis=1)\n",
        "geco_fixProp = pd.read_csv(local_dir+'zuco_featureset-geco_fixProp_word_level.csv').drop(['sentence_id','word_id', 'word'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-IQK1vvr7nX"
      },
      "source": [
        "test_geco_FFD = pd.read_csv(local_dir+'zuco_testset-geco_FFD_word_level.csv').drop(['sentence_id','word_id', 'word'], axis=1)\n",
        "test_geco_TRT = pd.read_csv(local_dir+'zuco_testset-geco_TRT_word_level.csv').drop(['sentence_id','word_id', 'word'], axis=1)\n",
        "test_geco_GPT = pd.read_csv(local_dir+'zuco_testset-geco_GPT_word_level.csv').drop(['sentence_id','word_id', 'word'], axis=1)\n",
        "test_geco_nFix = pd.read_csv(local_dir+'zuco_testset-geco_nFix_word_level.csv').drop(['sentence_id','word_id', 'word'], axis=1)\n",
        "test_geco_fixProp = pd.read_csv(local_dir+'zuco_testset-geco_fixProp_word_level.csv').drop(['sentence_id','word_id', 'word'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1_5UoDar7nX"
      },
      "source": [
        "data['geco_FFD_mean'] = geco_FFD['mean']\n",
        "data['geco_FFD_median'] = geco_FFD['median']\n",
        "data['geco_FFD_std'] = geco_FFD['std']\n",
        "\n",
        "data['geco_TRT_mean'] = geco_TRT['mean']\n",
        "data['geco_TRT_median'] = geco_TRT['median']\n",
        "data['geco_TRT_std'] = geco_TRT['std']\n",
        "\n",
        "data['geco_GPT_mean'] = geco_GPT['mean']\n",
        "data['geco_GPT_median'] = geco_GPT['median']\n",
        "data['geco_GPT_std'] = geco_GPT['std']\n",
        "\n",
        "data['geco_nFix_mean'] = geco_nFix['mean']\n",
        "data['geco_nFix_median'] = geco_nFix['median']\n",
        "data['geco_nFix_std'] = geco_nFix['std']\n",
        "\n",
        "data['geco_fixProp_mean'] = geco_fixProp['mean']\n",
        "data['geco_fixProp_median'] = geco_fixProp['median']\n",
        "data['geco_fixProp_std'] = geco_fixProp['std']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypgeuwSfr7nY"
      },
      "source": [
        "test_data['geco_FFD_mean'] = test_geco_FFD['mean']\n",
        "test_data['geco_FFD_median'] = test_geco_FFD['median']\n",
        "test_data['geco_FFD_std'] = test_geco_FFD['std']\n",
        "\n",
        "test_data['geco_TRT_mean'] = test_geco_TRT['mean']\n",
        "test_data['geco_TRT_median'] = test_geco_TRT['median']\n",
        "test_data['geco_TRT_std'] = test_geco_TRT['std']\n",
        "\n",
        "test_data['geco_GPT_mean'] = test_geco_GPT['mean']\n",
        "test_data['geco_GPT_median'] = test_geco_GPT['median']\n",
        "test_data['geco_GPT_std'] = test_geco_GPT['std']\n",
        "\n",
        "test_data['geco_nFix_mean'] = test_geco_nFix['mean']\n",
        "test_data['geco_nFix_median'] = test_geco_nFix['median']\n",
        "test_data['geco_nFix_std'] = test_geco_nFix['std']\n",
        "\n",
        "test_data['geco_fixProp_mean'] = test_geco_fixProp['mean']\n",
        "test_data['geco_fixProp_median'] = test_geco_fixProp['median']\n",
        "test_data['geco_fixProp_std'] = test_geco_fixProp['std']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wugRq1mor7nY"
      },
      "source": [
        "data.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Td4hLH0J3ai"
      },
      "source": [
        "# Add features from previous/subsequent words\n",
        "\n",
        "print_vars = add_seq_states(n_forward=2, n_backward=2, vars=['Conc_M', 'Conc_SD', 'Percent_Known',\n",
        "       'is_mwe', 'mwe_cat', 'comp_score', 'comp_weights', \n",
        "       'CAT_pos', 'CAT_word', 'Is_EOS', 'Is_SOS', 'word_len', 'zipf_frequency',\n",
        "       'word_frequency', 'saccade_cat_binary', 'saccade_cat', 'is_strange',\n",
        "       'geco_FFD_mean', 'geco_FFD_median', 'geco_FFD_std', 'geco_TRT_mean',\n",
        "       'geco_TRT_median', 'geco_TRT_std', 'geco_GPT_mean', 'geco_GPT_median',\n",
        "       'geco_GPT_std', 'geco_nFix_mean', 'geco_nFix_median', 'geco_nFix_std',\n",
        "       'geco_fixProp_mean', 'geco_fixProp_median', 'geco_fixProp_std'])\n",
        "print('Added:')\n",
        "print('\\'' + '\\', \\''.join(print_vars) + '\\'')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XAuVgFIr7nY"
      },
      "source": [
        "print_vars = add_seq_states_test(n_forward=2, n_backward=2, vars=['Conc_M', 'Conc_SD', 'Percent_Known',\n",
        "       'is_mwe', 'mwe_cat', 'comp_score', 'comp_weights', \n",
        "       'CAT_pos', 'CAT_word', 'Is_EOS', 'Is_SOS', 'word_len', 'zipf_frequency',\n",
        "       'word_frequency', 'saccade_cat_binary', 'saccade_cat', 'is_strange',\n",
        "       'geco_FFD_mean', 'geco_FFD_median', 'geco_FFD_std', 'geco_TRT_mean',\n",
        "       'geco_TRT_median', 'geco_TRT_std', 'geco_GPT_mean', 'geco_GPT_median',\n",
        "       'geco_GPT_std', 'geco_nFix_mean', 'geco_nFix_median', 'geco_nFix_std',\n",
        "       'geco_fixProp_mean', 'geco_fixProp_median', 'geco_fixProp_std'])\n",
        "print('Added:')\n",
        "print('\\'' + '\\', \\''.join(print_vars) + '\\'')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyP7t-5mr7nY"
      },
      "source": [
        "use_features = ['word_len', 'saccade_cat_binary', 'saccade_cat','geco_FFD_mean',\n",
        "                'Is_EOS', 'geco_TRT_mean', 'Is_SOS', 'geco_nFix_median', \n",
        "                'word_frequency','PREV_2_Is_SOS', 'PREV_1_Is_SOS', \n",
        "                'PREV_1_geco_fixProp_median', 'PREV_1_Percent_Known', \n",
        "                'NEXT_1_word_len', 'PREV_1_word_len', 'PREV_1_comp_weights', \n",
        "                'PREV_2_comp_score', 'geco_GPT_median', 'NEXT_1_Is_EOS',\n",
        "                'is_mwe', 'NEXT_1_CAT_pos', 'PREV_1_word_frequency', \n",
        "                'PREV_1_CAT_word','PREV_1_saccade_cat','Conc_M', \n",
        "                'CAT_word', 'PREV_1_geco_nFix_mean', 'NEXT_1_word_frequency',\n",
        "                'CAT_pos', 'PREV_1_geco_FFD_std','comp_weights',\n",
        "                'PREV_2_CAT_pos','geco_fixProp_std', 'geco_nFix_std',\n",
        "                'PREV_2_geco_GPT_std','NEXT_2_saccade_cat_binary', \n",
        "                'NEXT_1_Conc_M','Percent_Known','PREV_1_geco_GPT_std', \n",
        "                'NEXT_2_geco_nFix_median', 'NEXT_2_word_frequency', \n",
        "                'NEXT_2_zipf_frequency', 'PREV_1_CAT_pos', \n",
        "                'PREV_2_mwe_cat', 'geco_fixProp_mean', 'PREV_1_is_strange', \n",
        "                'NEXT_1_geco_GPT_std', 'mwe_cat', 'PREV_1_is_mwe',\n",
        "                'geco_FFD_std','NEXT_1_mwe_cat', 'NEXT_1_Percent_Known', \n",
        "                'PREV_1_geco_fixProp_std', 'is_strange',\n",
        "                'PREV_1_geco_nFix_std', 'PREV_1_Conc_M', \n",
        "                'NEXT_1_geco_TRT_std', 'geco_GPT_std','NEXT_1_geco_fixProp_std',\n",
        "                'NEXT_2_geco_fixProp_std', 'PREV_1_geco_nFix_median',\n",
        "                'NEXT_2_is_strange', 'NEXT_2_Conc_M', 'NEXT_2_geco_GPT_std', \n",
        "                'NEXT_1_comp_weights','PREV_2_geco_GPT_mean','PREV_2_is_strange', \n",
        "                'PREV_1_geco_TRT_std', 'PREV_1_Conc_SD', 'comp_score', \n",
        "                'NEXT_2_comp_weights', 'Conc_SD', 'NEXT_2_comp_score',\n",
        "                'NEXT_1_geco_nFix_median', 'zipf_frequency', \n",
        "                'PREV_2_CAT_word', 'NEXT_1_CAT_word','NEXT_2_Conc_SD', \n",
        "                'PREV_2_word_frequency','NEXT_1_Conc_SD','NEXT_1_geco_FFD_std',\n",
        "                'NEXT_1_is_strange', 'NEXT_1_saccade_cat_binary',\n",
        "                'PREV_2_geco_fixProp_std', 'NEXT_2_Is_EOS', 'w2v_embedding']\n",
        "\n",
        "test_data = test_data.replace('.',0, regex=True)\n",
        "data = data.replace('.', 0, regex=True)\n",
        "test_data.NEXT_2_comp_weights = test_data.NEXT_2_comp_weights.astype(float)\n",
        "\n",
        "\n",
        "X = np.vstack([list(flatten(x)) for x in data[use_features].values]).tolist()      \n",
        "\n",
        "#X_test =  np.vstack([list(flatten(x)) for x in test_data[use_features].values]).tolist()      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0KrSlaTr7nZ"
      },
      "source": [
        "data.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIy_vjUfn-2F"
      },
      "source": [
        "# Build the task for test data\n",
        "\n",
        "X_train, y_train = X[:USE_SPLIT[0]]+X[USE_SPLIT[1]:], data[['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']].drop(data.index[USE_SPLIT[0]:USE_SPLIT[1]])\n",
        "X_val, y_val = X[USE_SPLIT[0]:USE_SPLIT[1]], data[['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']][USE_SPLIT[0]:USE_SPLIT[1]]\n",
        "val_df = data[USE_SPLIT[0]:USE_SPLIT[1]]\n",
        "#X_train, y_train = X, data[['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXdVJL-dzCmE"
      },
      "source": [
        "# Build the task for paper experiments\n",
        "\n",
        "# Define splits\n",
        "\n",
        "SPLIT_1 = (7839,10490,400,533)\n",
        "SPLIT_2 = (10490,13082,533,667)\n",
        "SPLIT_3 = (13082,15737,667,800)\n",
        "SPLIT_ALL = (15737,15737,800,800)\n",
        "\n",
        "USE_SPLIT = SPLIT_3\n",
        "\n",
        "#model_dir = shared_dir+'Final models/no_BERT_split_3/'\n",
        "#model_dir = shared_dir+'Final models/split_2/'\n",
        "#model_dir = shared_dir+'Final models/split_3/'\n",
        "\n",
        "\n",
        "X_train, y_train = X[:USE_SPLIT[0]]+X[USE_SPLIT[1]:], data[['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']].drop(data.index[USE_SPLIT[0]:USE_SPLIT[1]])\n",
        "X_val, y_val = X[USE_SPLIT[0]:USE_SPLIT[1]], data[['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']][USE_SPLIT[0]:USE_SPLIT[1]]\n",
        "val_df = data[USE_SPLIT[0]:USE_SPLIT[1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68AsMTQO5duQ"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train_scale = scaler.transform(X_train)\n",
        "#X_test_scale = scaler.transform(X_test)\n",
        "X_val_scale = scaler.transform(X_val)\n",
        "\n",
        "#X_train_scale = X_train\n",
        "#X_val_scale = X_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItvWj5_xr7nb"
      },
      "source": [
        "f = np.array(X_test_scale)\n",
        "\n",
        "f[np.argwhere(np.isnan(f))] = 0\n",
        "X_test_scale = f.tolist()\n",
        "print(np.argwhere(np.isinf(f)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqP_i1HT3rhU"
      },
      "source": [
        "f = np.array(X_val_scale)\n",
        "\n",
        "f[np.argwhere(np.isnan(f))] = 0\n",
        "X_val_scale = f.tolist()\n",
        "print(np.argwhere(np.isinf(f)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEa3tOv8et8s"
      },
      "source": [
        "from sklearn.linear_model import ElasticNetCV\n",
        "\n",
        "preds = {}\n",
        "total_mae = 0\n",
        "\n",
        "for target in tqdm(['nFix']):\n",
        "    print('fitting', target)\n",
        "    target_y_train = y_train[target]\n",
        "    target_y_val = y_val[target] \n",
        "    regr = ElasticNetCV(max_iter=200, l1_ratio=[0.999999,1])\n",
        "    regr.fit(X_train_scale, target_y_train)\n",
        "    print('alpha', regr.alpha_)\n",
        "    print('l1_ratio', regr.l1_ratio_)\n",
        "    #y_hat = regr.predict(X_val_scale)\n",
        "    #preds[target] = y_hat\n",
        "    #print(target, 'MAE', mean_absolute_error(y_val[target], preds[target]))  \n",
        "    #filename = model_dir+(target)+'.pkl'\n",
        "    #pickle.dump(regr, open(filename,'wb'))\n",
        "\n",
        "#total_mae /= 5\n",
        "#print(f'Total MAE {total_mae}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--4z2W39foC_"
      },
      "source": [
        "coefs_abs = np.abs(regr.coef_)\n",
        "coefs_normed = 100*coefs_abs/np.sum(coefs_abs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-XXqZ3Ogr_8"
      },
      "source": [
        "sum(coefs_normed[85:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiUud7UlfuS-"
      },
      "source": [
        "for name, val in zip(feature_locs[:85], coefs_normed[:85]):\n",
        "  print(name.replace('_','-'),val, '\\\\\\\\') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_WDGPM2ur7nb"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "\n",
        "preds = {}\n",
        "total_mae = 0\n",
        "\n",
        "for target in tqdm(['nFix','FFD','GPT', 'TRT', 'fixProp']):\n",
        "    print('fitting', target)\n",
        "    target_y_train = y_train[target]\n",
        "    target_y_val = y_val[target] \n",
        "    regr = RandomForestRegressor(max_depth=7, n_estimators=100, max_features=None)\n",
        "    regr.fit(X_train_scale, target_y_train)\n",
        "    y_hat = regr.predict(X_val_scale)\n",
        "    preds[target] = y_hat\n",
        "    print(target, 'MAE', mean_absolute_error(y_val[target], preds[target]))  \n",
        "\n",
        "    r = permutation_importance(regr, X_val_scale, y_val[target], n_repeats=10, random_state=0)\n",
        "    print('permuatations calculated')\n",
        "    normed_importances_mean = 100*r.importances_mean/np.sum(r.importances_mean)\n",
        "\n",
        "    print(f'w2v-embedding-all', np.sum(normed_importances_mean[85:385]))\n",
        "    \n",
        "    for i in r.importances_mean.argsort()[::-1]:\n",
        "      if r.importances_mean[i] > 0:\n",
        "         if i < 85 or i >= 385:\n",
        "           print(feature_locs[i].replace('_','-'), f\"{normed_importances_mean[i]:.5f}\")\n",
        "\n",
        "    print('____________________________\\n\\n\\n')\n",
        "\n",
        "total_mae /= 5\n",
        "print(f'Total MAE {total_mae}')\n",
        "\n",
        "#WITH SPLIT = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFF973x-7VyF"
      },
      "source": [
        "total_mae = 0\n",
        "for target in ['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']:\n",
        "      target_y_val = y_val[target] \n",
        "\n",
        "      preds[target][preds[target]>100] = 100\n",
        "      preds[target][preds[target]<0] = 0\n",
        "      print(target, 'MAE', mean_absolute_error(y_val[target], preds[target]))  \n",
        "      total_mae += mean_absolute_error(y_val[target], preds[target])\n",
        "\n",
        "total_mae /= 5\n",
        "print(f'Total MAE {total_mae}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E7zZYivEXTM"
      },
      "source": [
        "regr.get_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuSmF-Yur7na"
      },
      "source": [
        "#obtain list of which feature is where for discovering elasticnet zeros \n",
        "feature_locs = [] \n",
        "for feat in use_features: \n",
        "    if type(data[feat][0]) is list:\n",
        "        print('start', len(feature_locs))\n",
        "        for i, x in enumerate(data[feat][0]):\n",
        "            feature_locs.append(str(feat)+'_'+str(i))\n",
        "        print('end', len(feature_locs))\n",
        "    else:\n",
        "        feature_locs.append(str(feat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-vLM_M7W-Ia"
      },
      "source": [
        "feature_locs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2c7uP68ELQm"
      },
      "source": [
        "filename = 'no_BERT_split1.pkl'\n",
        "pickle.dump(regr, open(filename,'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzScrKdWEZLI"
      },
      "source": [
        "model = pickle.load(open(model_dir+'TRT.pkl', 'rb'))\n",
        "temp = model.predict(X_val_scale)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75XabczDWuET"
      },
      "source": [
        "importance_dict = {feat:val for feat, val in zip(feature_locs,model.feature_importances_)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AYzVpEoXPsX"
      },
      "source": [
        "from collections import Counter\n",
        "features_ordered = Counter(importance_dict).most_common()\n",
        "for i, (k,v) in enumerate(features_ordered):\n",
        "  print(k, v)\n",
        "  if i >20:\n",
        "    break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUqY1A1cEozN"
      },
      "source": [
        "mean_absolute_error(y_val['fixProp'], temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YESadC1Cr7nb"
      },
      "source": [
        "clean_test_data = pd.read_csv('test_data_with_pos.csv')\n",
        "\n",
        "\n",
        "submit_data=clean_test_data[['sentence_id','word_id', 'word']]\n",
        "for pred_target in ['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']:\n",
        "    submit_data[pred_target] = safe_preds[pred_target]\n",
        "    \n",
        "    \n",
        "submit_data.to_csv('answer.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1whbxHxr7nb"
      },
      "source": [
        "answers = pd.read_csv('answer.txt', index_col=0)\n",
        "answers.to_csv('answer2.txt', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "037khRcEr7nb"
      },
      "source": [
        "from copy import deepcopy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6p2Ea7Mr7nb"
      },
      "source": [
        "safe_preds = deepcopy(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-oT1UW6r7nc"
      },
      "source": [
        "for pred_target in ['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']:\n",
        "    safe_preds[pred_target][safe_preds[pred_target]>100] = 100\n",
        "    safe_preds[pred_target][safe_preds[pred_target]<0] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTPKzm0er7nc"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "total_mae = 0\n",
        "for target in tqdm(['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']):\n",
        "    print('fitting', target)\n",
        "    target_y_train = y_train[target]\n",
        "    target_y_val = y_val[target] \n",
        "    regr = RandomForestRegressor(max_depth=7, random_state=0, verbose=2)\n",
        "    regr.fit(X_train_scale, target_y_train)\n",
        "    y_hat = regr.predict(X_val_scale)\n",
        "\n",
        "    preds[target] = y_hat\n",
        "    print(target, 'MAE', mean_absolute_error(y_val[target], preds[target]))  \n",
        "    total_mae += mean_absolute_error(y_val[target], preds[target])\n",
        "#total_mae /= 5\n",
        "#print(f'Total MAE {total_mae}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b45x-s7Tr7nc"
      },
      "source": [
        "regr.predict(X_val_scale)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUxYc0UEr7nc"
      },
      "source": [
        "y_hat = regr.predict(X_val_scale)\n",
        "\n",
        "mean_absolute_error(y_hat, y_val['fixProp'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgnFcb_Lr7nc"
      },
      "source": [
        "model = ElasticNetCV\n",
        "\n",
        "preds = {}\n",
        "total_mae = 0\n",
        "for target in tqdm(['nFix','FFD','GPT', 'TRT', 'fixProp']):\n",
        "    print('fitting', target)\n",
        "    target_y_train = y_train[target]\n",
        "    target_y_val = y_val[target] \n",
        "\n",
        "    reg = model(max_iter=2000, l1_ratio=[.1, .5, .6, .7, .9, .95, .99, 1]).fit(X_train_scale, target_y_train)\n",
        "    print('alpha', reg.alpha_)\n",
        "    print('l1_ratio',reg.l1_ratio_)\n",
        "    y_hat = reg.predict(X_val_scale)\n",
        "\n",
        "    preds[target] = y_hat\n",
        "    print(target, 'MAE', mean_absolute_error(y_val[target], preds[target]))  \n",
        "    total_mae += mean_absolute_error(y_val[target], preds[target])\n",
        "total_mae /= 5\n",
        "print(f'Total MAE {total_mae}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnJjxIn0r7nc"
      },
      "source": [
        "print('percentage of features used by ElasticNet:', 100 * np.count_nonzero(reg.coef_)/len(reg.coef_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZFMm91Zr7nc"
      },
      "source": [
        "w2v_dims_prev_1 = [] \n",
        "w2v_dims_prev_2 = []\n",
        "w2v_dims_current = []\n",
        "w2v_dims_next_1 = []\n",
        "w2v_dims_next_2 = [] \n",
        "w2v_dims = 0 \n",
        "from collections import Counter \n",
        "\n",
        "coef_dict = {} \n",
        "total_coef = np.sum(np.abs(reg.coef_))\n",
        "for loc in np.argwhere(reg.coef_):\n",
        "    current = feature_locs[int(loc)]\n",
        "#  if current[:10] == 'NEXT_2_w2v':\n",
        "  #      w2v_dims_next_2.append(current.split('_')[-1])\n",
        " #   elif current[:10] == 'NEXT_1_w2v':\n",
        "  #      w2v_dims_next_1.append(current.split('_')[-1])\n",
        "    if current[:3] == 'w2v':\n",
        "        w2v_dims+=1\n",
        "  #  elif current[:10] == 'PREV_1_w2v':\n",
        "  #      w2v_dims_prev_1.append(current.split('_')[-1])\n",
        "  #  elif current[:10] == 'PREV_2_w2v':\n",
        "   #     w2v_dims_prev_2.append(current.split('_')[-1])\n",
        "    else: \n",
        "       # print(feature_locs[int(loc)], reg.coef_[int(loc)]*100/total_coef)\n",
        "        coef_dict[feature_locs[int(loc)]] = np.abs(reg.coef_[int(loc)]*100/total_coef)\n",
        "\n",
        "print('w2v dims used:', w2v_dims)\n",
        "print(Counter(coef_dict).most_common())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qZClwSlr7nd"
      },
      "source": [
        "new_feats = [feat for (feat, weight) in Counter(coef_dict).most_common() if weight > 0.1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rbw4BYZr7nd"
      },
      "source": [
        "' '.join(new_feats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vw2gPdi3xcS"
      },
      "source": [
        "total_mae = 0\n",
        "for target in ['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']:\n",
        "      target_y_train = y_train[target]\n",
        "      target_y_val = y_val[target] \n",
        "\n",
        "      preds[target][preds[target]>100] = 100\n",
        "      preds[target][preds[target]<0] = 0\n",
        "      print(target, 'MAE with cutoffs', mean_absolute_error(y_val[target], preds[target]))  \n",
        "      total_mae += mean_absolute_error(y_val[target], preds[target])\n",
        "\n",
        "total_mae /= 5\n",
        "print(f'Total MAE with cutoffs {total_mae}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRScbUcor7nd"
      },
      "source": [
        "from sklearn.linear_model import BayesianRidge\n",
        "def train_from_feature_list(feature_columns):\n",
        "    print(feature_columns)\n",
        "    X = np.vstack([list(flatten(x)) for x in data[feature_columns].values]).tolist()        \n",
        "   \n",
        "    # Build the task\n",
        "\n",
        "    X_train, y_train = X[:USE_SPLIT[0]]+X[USE_SPLIT[1]:], data[['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']].drop(data.index[USE_SPLIT[0]:USE_SPLIT[1]])\n",
        "    X_val, y_val = X[USE_SPLIT[0]:USE_SPLIT[1]], data[['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']][USE_SPLIT[0]:USE_SPLIT[1]]\n",
        "    val_df = data[USE_SPLIT[0]:USE_SPLIT[1]]\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    scaler.fit(X_train)\n",
        "\n",
        "    X_train_scale = scaler.transform(X_train)\n",
        "    X_val_scale = scaler.transform(X_val)\n",
        "\n",
        "    model = BayesianRidge\n",
        "\n",
        "    preds = {}\n",
        "    total_mae = 0\n",
        "    for target in ['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']:\n",
        "      print('fitting', target)\n",
        "      target_y_train = y_train[target]\n",
        "      target_y_val = y_val[target] \n",
        "\n",
        "      reg = model().fit(X_train_scale, target_y_train)  \n",
        "\n",
        "      y_hat = reg.predict(X_val_scale)\n",
        "      y_hat[y_hat>100] = 100\n",
        "      y_hat[y_hat<0] = 0\n",
        "\n",
        "      preds[target] = y_hat\n",
        "      print(target, 'MAE', mean_absolute_error(y_val[target], preds[target]))  \n",
        "      total_mae += mean_absolute_error(y_val[target], preds[target])\n",
        "\n",
        "    total_mae /= 5\n",
        "    print(f'Total MAE {total_mae}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPCVYtE0_vML"
      },
      "source": [
        "# Run the task with the model specified\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "\n",
        "\n",
        "model = BayesianRidge\n",
        "\n",
        "preds = {}\n",
        "total_mae = 0\n",
        "for target in ['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']:\n",
        "  print('fitting', target)\n",
        "  target_y_train = y_train[target]\n",
        "  target_y_val = y_val[target] \n",
        "\n",
        "  #reg = model(max_iter=20000, l1_ratio=0.6).fit(X_train_scale, target_y_train)\n",
        "  reg = model().fit(X_train_scale, target_y_train)  \n",
        "\n",
        "  #reg = model(alpha_2=1e-2).fit(X_train_scale, target_y_train)\n",
        "  #print('alpha', reg.alpha_)\n",
        "  #print('l1_ratio', reg.alpha_)\n",
        "  y_hat = reg.predict(X_val_scale)\n",
        "  #y_hat[y_hat>100] = 100\n",
        "  #y_hat[y_hat<0] = 0\n",
        "\n",
        "  preds[target] = y_hat\n",
        "  print(target, 'MAE', mean_absolute_error(y_val[target], preds[target]))  \n",
        "  total_mae += mean_absolute_error(y_val[target], preds[target])\n",
        "\n",
        "total_mae /= 5\n",
        "print(f'Total MAE {total_mae}')\n",
        "\n",
        "'''\n",
        "model = ElasticNetCV\n",
        "\n",
        "\n",
        "for l1ratio in [.1, .5, .7, .9, .95, .99, 1]:\n",
        "    print('_________________')\n",
        "    print(l1ratio)\n",
        "    preds = {}\n",
        "    total_mae = 0\n",
        "    for target in ['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']:\n",
        "      print('fitting', target)\n",
        "      target_y_train = y_train[target]\n",
        "      target_y_val = y_val[target] \n",
        "\n",
        "      reg = model(max_iter=2000, l1_ratio=l1ratio).fit(X_train_scale, target_y_train)\n",
        "      \n",
        "      y_hat = reg.predict(X_val_scale)\n",
        "\n",
        "      preds[target] = y_hat\n",
        "      print(target, 'MAE', mean_absolute_error(y_val[target], preds[target]))  \n",
        "      total_mae += mean_absolute_error(y_val[target], preds[target])\n",
        "\n",
        "    total_mae /= 5\n",
        "    print(f'Total MAE {total_mae}')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8SbYZNX5t5A"
      },
      "source": [
        "data.head"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWao8u7Gz4_D"
      },
      "source": [
        "fitting nFix\n",
        "nFix MAE 3.897349478994872\n",
        "fitting FFD\n",
        "FFD MAE 0.6865561010487296\n",
        "fitting GPT\n",
        "GPT MAE 2.2355537777376155\n",
        "fitting TRT\n",
        "TRT MAE 1.5097279541778272\n",
        "fitting fixProp\n",
        "fixProp MAE 11.277121487697162\n",
        "Total MAE 3.921261759931241"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Zrq7ORaF-id"
      },
      "source": [
        "# Run the task with the model specified\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "\n",
        "model = MLPRegressor\n",
        "\n",
        "\n",
        "preds = {}\n",
        "total_mae = 0\n",
        "for target in ['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']:\n",
        "  print('fitting', target)\n",
        "  target_y_train = y_train[target]\n",
        "  target_y_val = y_val[target] \n",
        "\n",
        "  reg = model(hidden_layer_sizes=500, alpha=1.00E-12, learning_rate='constant', batch_size=400, learning_rate_init=1.00E-06, max_iter=10000).fit(X_train_scale, target_y_train)\n",
        "  #reg = model(alpha_2=1e-2).fit(X_train_scale, target_y_train)\n",
        "  y_hat = reg.predict(X_val_scale)\n",
        "\n",
        "  preds[target] = y_hat\n",
        "  print(target, 'MAE', mean_absolute_error(y_val[target], preds[target]))  \n",
        "  total_mae += mean_absolute_error(y_val[target], preds[target])\n",
        "\n",
        "total_mae /= 5\n",
        "print(f'Total MAE {total_mae}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FerGCDDm6wTd"
      },
      "source": [
        "from sklearn.linear_model import MultiTaskElasticNetCV\n",
        "\n",
        "model = MultiTaskElasticNetCV\n",
        "\n",
        "preds = {}\n",
        "total_mae = 0\n",
        "print('fitting', ['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp'])\n",
        "target_y_train = y_train[['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']].values\n",
        "target_y_val = y_val[['nFix',\t'FFD',\t'GPT', 'TRT', 'fixProp']] \n",
        "\n",
        "\n",
        "reg = model(max_iter=2000, l1_ratio=0.5).fit(X_train_scale, target_y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpw1sPM4RVx1"
      },
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump([val_df, preds], open('drive/Shareddrives/CMCL Shared Task - CogNLP@Sheffield/Data/CMCL XLNET Preds Split 3.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd94zUJDSVEn"
      },
      "source": [
        "val_df_1, preds_1 = pickle.load(open('drive/Shareddrives/CMCL Shared Task - CogNLP@Sheffield/Data/CMCL XLNET Preds Split 1.pkl', 'rb'))\n",
        "val_df_2, preds_2 = pickle.load(open('drive/Shareddrives/CMCL Shared Task - CogNLP@Sheffield/Data/CMCL XLNET Preds Split 2.pkl', 'rb'))\n",
        "val_df_3, preds_3 = pickle.load(open('drive/Shareddrives/CMCL Shared Task - CogNLP@Sheffield/Data/CMCL XLNET Preds Split 3.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br5XQjrHSkOz"
      },
      "source": [
        "val_df_all = val_df_1.append(val_df_2, ignore_index=True).append(val_df_3, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFh9Cg5KS0S2"
      },
      "source": [
        "preds_all = {}\n",
        "\n",
        "for key in preds_1:\n",
        "    preds_all[key] = np.hstack((preds_1[key], preds_2[key], preds_3[key]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-U_EhTfbm4f"
      },
      "source": [
        "# Format validation results \n",
        "val_results, val_store = val_to_plot(val_df_all, preds_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO_35y38BJSy"
      },
      "source": [
        "pred_review = pd.DataFrame()\n",
        "\n",
        "# Plot validation results from worst to best\n",
        "sent_word2mae = {}\n",
        "\n",
        "\n",
        "'''\n",
        "for val_idx, score in Counter(val_results).most_common():\n",
        "    x, y, y_hat = val_store[val_idx]\n",
        "    print(' '.join(x))\n",
        "    print('MAE:', mean_absolute_error(y, y_hat))\n",
        "\n",
        "    for idx, word in enumerate(x):\n",
        "      print(word, np.abs(y[idx,:]-y_hat[idx,:]))\n",
        "      print('MAE:', mean_absolute_error(y[idx,:], y_hat[idx,:]))\n",
        "    #plot_sent(x, y, y_hat)\n",
        "      print()\n",
        "'''\n",
        "for val_idx, score in Counter(val_results).most_common():\n",
        "    x, y, y_hat = val_store[val_idx]\n",
        "\n",
        "    for idx, word in enumerate(x):\n",
        "      sent_word2mae[str(val_idx)+'_'+str(idx)] = mean_absolute_error(y[idx,:], y_hat[idx,:])\n",
        "\n",
        "worst_words = {k:e for e, (k, v) in enumerate(Counter(sent_word2mae).most_common())}\n",
        "\n",
        "\n",
        "words = []\n",
        "word_idx = []\n",
        "word_badness = []\n",
        "sent_badness = []\n",
        "sent_mae = []\n",
        "sent_nFix\t= []\n",
        "sent_FFD = []\n",
        "sent_GPT = []\n",
        "sent_TRT = []\n",
        "sent_fixProp = []\n",
        "word_mae = []\n",
        "word_nFix\t= []\n",
        "word_FFD = []\n",
        "word_GPT = []\n",
        "word_TRT = []\n",
        "word_fixProp = []\n",
        "\n",
        "\n",
        "for sent_b, (val_idx, score) in enumerate(Counter(val_results).most_common()):\n",
        "    x, y, y_hat = val_store[val_idx]\n",
        "    sent_badness.extend([sent_b]*len(x))#+[''])\n",
        "    sent_mae.extend([\"{:.3f}\".format(score)]*len(x))#+[''])\n",
        "    sent_nFix.extend([\"{:.3f}\".format(mean_absolute_error(y[:,0], y_hat[:,0]))]*len(x))#+[''])\n",
        "    sent_FFD.extend([\"{:.3f}\".format(mean_absolute_error(y[:,1], y_hat[:,1]))]*len(x))#+[''])\n",
        "    sent_GPT.extend([\"{:.3f}\".format(mean_absolute_error(y[:,2], y_hat[:,2]))]*len(x))#+[''])\n",
        "    sent_TRT.extend([\"{:.3f}\".format(mean_absolute_error(y[:,3], y_hat[:,3]))]*len(x))#+[''])\n",
        "    sent_fixProp.extend([\"{:.3f}\".format(mean_absolute_error(y[:,4], y_hat[:,4]))]*len(x))#+[''])\n",
        "\n",
        "    for idx, word in enumerate(x):\n",
        "        word_idx.append(idx)\n",
        "        words.append(word)\n",
        "        word_mae.extend([\"{:.3f}\".format(mean_absolute_error(y[idx,:], y_hat[idx,:]))])\n",
        "        word_nFix.extend([\"{:.3f}\".format((y_hat[idx,0]-y[idx,0]))])\n",
        "        word_FFD.extend([\"{:.3f}\".format((y_hat[idx,1]-y[idx,1]))])\n",
        "        word_GPT.extend([\"{:.3f}\".format((y_hat[idx,2]-y[idx,2]))])\n",
        "        word_TRT.extend([\"{:.3f}\".format((y_hat[idx,3]-y[idx,3]))])\n",
        "        word_fixProp.extend([\"{:.3f}\".format((y_hat[idx,4]-y[idx,4]))])\n",
        "        word_badness.append(worst_words[str(val_idx)+'_'+str(idx)])\n",
        "    '''\n",
        "    word_idx.append('')\n",
        "    words.append('')\n",
        "    word_mae.extend([''])\n",
        "    word_nFix.extend([''])\n",
        "    word_FFD.extend([''])\n",
        "    word_GPT.extend([''])\n",
        "    word_TRT.extend([''])\n",
        "    word_fixProp.extend([''])\n",
        "    word_badness.extend([''])\n",
        "    '''\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAwnUkjpPms6"
      },
      "source": [
        "#words,word_idx,word_badness,sent_mae,sent_nFix,sent_FFD,sent_GPT,sent_TRT,sent_fixProp,word_mae,word_nFix,word_FFD,word_GPT,word_TRT,word_fixProp\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtdYjUl0MNPi"
      },
      "source": [
        "df = pd.DataFrame(list(zip(word_idx,words,word_badness,word_mae,word_nFix,word_FFD,word_GPT,word_TRT,word_fixProp,sent_badness,sent_mae,sent_nFix,sent_FFD,sent_GPT,sent_TRT,sent_fixProp\n",
        ")), columns=['word_idx', 'words', 'word_badness', 'word_mae', 'word_nFix', 'word_FFD', 'word_GPT', 'word_TRT', 'word_fixProp', 'sent_badness', 'sent_mae', 'sent_nFix', 'sent_FFD', 'sent_GPT', 'sent_TRT', 'sent_fixProp']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl0SlSFSLvw6"
      },
      "source": [
        "df.to_excel(\"drive/Shareddrives/CMCL Shared Task - CogNLP@Sheffield/Data/All Preds.xlsx\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1FaKVatTm9O"
      },
      "source": [
        "plt.hist(preds['fixProp'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP7dl-R2m0Ei"
      },
      "source": [
        "plt.hist(y_val['fixProp'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfJiuuqnm1vq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}